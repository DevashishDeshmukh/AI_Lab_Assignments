import numpy as np
import matplotlib.pyplot as plt

class NODE:
    def __init__(self, reward, correctProb):
        self.REWARD = reward
        self.PROBABILITY = correctProb
        self.next = None

class AGENT:
    def __init__(self):
        self.N = 10
        self.THETA = 0.0001
        self.gamma = 0.9
        self.action = ["CONTINUE", "QUIT"]
        
        rewards = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000, 5000000]
        correct_ans_Prob = [0.99, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]
        
        START_NODE = NODE(-1,0)
        TEMP = START_NODE
        
        for i in range(self.N):  
            TEMP.next = NODE(rewards[i],correct_ans_Prob[i])
            TEMP = TEMP.next
        
        self.S1 = START_NODE.next        


class MarkovDecisionProcess:
    def __init__(self):
        self.agent = AGENT()
        self.valueFun = {s: 0 for s in range(self.agent.N)}
        self.iter = 0
        self.enteredTimes = {s: 0 for s in range(self.agent.N)}
        self.endCaseTester = False
        self.PLOT_STATES = [x for x in range(self.agent.N)]
    
    def borower(self, state, iter):
        if(state == None): return 0
        self.enteredTimes[iter] += 1
    
        OLD_VALUE = self.valueFun[iter]
        reward_we_got = 0
        if iter == 0:
            QUIT_REWARD = 0
        else:
            QUIT_REWARD = self.valueFun[iter-1]
        
        ANSWER = np.random.rand()
        
        if ANSWER <= state.PROBABILITY:
            reward_we_got = state.PROBABILITY * (state.REWARD + (self.agent.gamma * self.borower(state.next, iter+1)))
            self.valueFun[iter] = (self.valueFun[iter] * self.enteredTimes[iter] + reward_we_got)/(self.enteredTimes[iter]+1)
            
            if(abs(self.valueFun[iter] - OLD_VALUE) < self.agent.THETA):
                self.endCaseTester = True

        return max(QUIT_REWARD, reward_we_got)
    
    def initiator(self):
        while self.endCaseTester == False:
            self.iter += 1
            HEAD = self.agent.S1
            self.borower(HEAD, 0)
        
        print("Total iter Performed : ", self.iter)
        print("Value function created  :")
        print(self.valueFun)
        
        for i in range(self.agent.N):
            self.enteredTimes[i] = (self.enteredTimes[i] / self.iter) * 100
        
        print("no of times entered:")
        print(self.enteredTimes)
        
        ExpectedValue = 0
        for i in range(self.agent.N):
            ExpectedValue = ExpectedValue + ((self.enteredTimes[i]/100) * self.valueFun[i])
        
        print("Reward Expected Value : ", ExpectedValue)
        

MarkovDecisionProcess().initiator()
     
