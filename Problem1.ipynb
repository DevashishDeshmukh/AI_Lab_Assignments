{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJDyaiCw5Ths64wKAX1oPE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DevashishDeshmukh/AI_Lab_Assignments/blob/main/Problem1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wuvet3UhF7x7",
        "outputId": "a5a7fcbb-7ef2-4dc9-a440-dc948209606c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total iter Performed :  1027\n",
            "Value function created  :\n",
            "{0: 7909.008184329472, 1: 8712.59149739412, 2: 10607.654598444615, 3: 15165.983330046563, 4: 21395.31265497945, 5: 41862.67992280426, 6: 52002.39656114303, 7: 68280.35830487414, 8: 127381.99076923078, 9: 0}\n",
            "no of times entered:\n",
            "{0: 100.0, 1: 99.31840311587146, 2: 89.77604673807205, 3: 70.88607594936708, 4: 50.04868549172347, 5: 30.671859785783834, 6: 15.579357351509252, 7: 6.329113924050633, 8: 1.4605647517039921, 9: 0.3894839337877313}\n",
            "Reward Expected Value :  74667.73088429323\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class NODE:\n",
        "    def __init__(self, reward, correctProb):\n",
        "        self.REWARD = reward\n",
        "        self.PROBABILITY = correctProb\n",
        "        self.next = None\n",
        "\n",
        "class AGENT:\n",
        "    def __init__(self):\n",
        "        self.N = 10\n",
        "        self.THETA = 0.0001\n",
        "        self.gamma = 0.9\n",
        "        self.action = [\"CONTINUE\", \"QUIT\"]\n",
        "        \n",
        "        rewards = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000, 5000000]\n",
        "        correct_ans_Prob = [0.99, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
        "        \n",
        "        START_NODE = NODE(-1,0)\n",
        "        TEMP = START_NODE\n",
        "        \n",
        "        for i in range(self.N):  \n",
        "            TEMP.next = NODE(rewards[i],correct_ans_Prob[i])\n",
        "            TEMP = TEMP.next\n",
        "        \n",
        "        self.S1 = START_NODE.next        \n",
        "\n",
        "\n",
        "class MarkovDecisionProcess:\n",
        "    def __init__(self):\n",
        "        self.agent = AGENT()\n",
        "        self.valueFun = {s: 0 for s in range(self.agent.N)}\n",
        "        self.iter = 0\n",
        "        self.enteredTimes = {s: 0 for s in range(self.agent.N)}\n",
        "        self.endCaseTester = False\n",
        "        self.PLOT_STATES = [x for x in range(self.agent.N)]\n",
        "    \n",
        "    def borower(self, state, iter):\n",
        "        if(state == None): return 0\n",
        "        self.enteredTimes[iter] += 1\n",
        "    \n",
        "        OLD_VALUE = self.valueFun[iter]\n",
        "        reward_we_got = 0\n",
        "        if iter == 0:\n",
        "            QUIT_REWARD = 0\n",
        "        else:\n",
        "            QUIT_REWARD = self.valueFun[iter-1]\n",
        "        \n",
        "        ANSWER = np.random.rand()\n",
        "        \n",
        "        if ANSWER <= state.PROBABILITY:\n",
        "            reward_we_got = state.PROBABILITY * (state.REWARD + (self.agent.gamma * self.borower(state.next, iter+1)))\n",
        "            self.valueFun[iter] = (self.valueFun[iter] * self.enteredTimes[iter] + reward_we_got)/(self.enteredTimes[iter]+1)\n",
        "            \n",
        "            if(abs(self.valueFun[iter] - OLD_VALUE) < self.agent.THETA):\n",
        "                self.endCaseTester = True\n",
        "\n",
        "        return max(QUIT_REWARD, reward_we_got)\n",
        "    \n",
        "    def initiator(self):\n",
        "        while self.endCaseTester == False:\n",
        "            self.iter += 1\n",
        "            HEAD = self.agent.S1\n",
        "            self.borower(HEAD, 0)\n",
        "        \n",
        "        print(\"Total iter Performed : \", self.iter)\n",
        "        print(\"Value function created  :\")\n",
        "        print(self.valueFun)\n",
        "        \n",
        "        for i in range(self.agent.N):\n",
        "            self.enteredTimes[i] = (self.enteredTimes[i] / self.iter) * 100\n",
        "        \n",
        "        print(\"no of times entered:\")\n",
        "        print(self.enteredTimes)\n",
        "        \n",
        "        ExpectedValue = 0\n",
        "        for i in range(self.agent.N):\n",
        "            ExpectedValue = ExpectedValue + ((self.enteredTimes[i]/100) * self.valueFun[i])\n",
        "        \n",
        "        print(\"Reward Expected Value : \", ExpectedValue)\n",
        "        \n",
        "\n",
        "MarkovDecisionProcess().initiator()\n",
        "     "
      ]
    }
  ]
}